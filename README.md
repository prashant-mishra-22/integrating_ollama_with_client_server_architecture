# ğŸ¤– Integrating Ollama with Client-Server Architecture  

This project demonstrates how to **integrate [Ollama](https://ollama.ai/)** (Large Language Model runtime) with a **custom TCP client-server architecture** in Python.  
It supports **user authentication, input filtering (emails/phone numbers/cuss words)**, and **streaming AI-generated responses** in real time.  

---

## âœ¨ Features
- ğŸ”‘ **User Authentication** (username & password check before connection)
- ğŸ§¹ **Message Filtering**
  - Client blocks **emails** and **phone numbers**
  - Server blocks **cuss words** (from `cuss_words.txt`)
- ğŸ¤ **Custom Protocol**
  - Fixed **64-byte headers** for message size
  - Chunked streaming of AI responses
- ğŸ§µ **Multi-client support** via threading
- ğŸ§  **Ollama-powered AI responses**
  - Uses `llama3.2` model by default

---

## ğŸ› ï¸ Tech Stack
- **Python 3.9+**
- **Socket Programming**
- **Threading**
- **Ollama Python Client**

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Install Requirements
Make sure you have [Ollama](https://ollama.ai/download) installed and running locally.  
Then install Python dependencies:

```bash
pip install ollama
````

### 2ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/prashant-mishra-22/integrating_ollama_with_client_server_architecture.git
cd integrating_ollama_with_client_server_architecture
```

### 3ï¸âƒ£ Start the Server

```bash
python s.py
```

Expected output:

```
[STARTING] server is starting ...
[LISTENING] server is listening on (your-ip, 5050)
```

### 4ï¸âƒ£ Start the Client

Open another terminal:

```bash
python c.py
```

Enter:

* Username: `****`
* Password: `****`

Then start chatting with the AI ğŸ¤–.
Type `<GoodBye>` to disconnect.

---

## ğŸ“‚ Project Structure

```
ğŸ“¦ integrating_ollama_with_client_server_architecture
 â”£ ğŸ“œ c.py              # Client code
 â”£ ğŸ“œ s.py              # Server code
 â”£ ğŸ“œ cuss_words.txt    # List of blocked words
 â”— ğŸ“œ README.md         # Documentation
```

---

## âš¡ Example Interaction

**Client Input:**

```
enter your message: Hello AI, how are you?
```

**Server (via Ollama Response):**

```
I'm doing great! How can I help you today?
```

---

## ğŸ”® Future Development

* ğŸ” **Secure Authentication**

  * Replace hardcoded username/password with database or JWT tokens.
* ğŸŒ **Unicode & Multilingual Support**

  * Switch from `ascii` â†’ `utf-8` to allow emojis and non-English text.
* ğŸ“¡ **WebSocket Integration**

  * Replace raw sockets with WebSockets for modern web compatibility.
* â˜ï¸ **Deployment**

  * Containerize with Docker, deploy on cloud (AWS/GCP/Azure).
* ğŸ›ï¸ **Admin Dashboard**

  * Track active users, blocked messages, and chat logs.
* âš™ï¸ **Better Moderation**

  * Integrate AI-powered toxicity detection instead of static `cuss_words.txt`.

---

## ğŸ™Œ Contribution

Contributions are welcome!

1. Fork the repo
2. Create a new branch
3. Commit your changes
4. Submit a PR ğŸš€

---

## ğŸ“œ License

This project is licensed under the MIT License â€“ feel free to use and modify.

---

### ğŸŒŸ If you found this useful, donâ€™t forget to **star â­ the repo**!

```

Do you want me to also **add code snippets (from `c.py` and `s.py`) inside the README** for quick reference, or keep it clean and minimal?
```
